
# ğŸ§  Multi-Head Attention Visualizer

Interactive Tool to Explore Attention Mechanisms in Transformer Models

---

## ğŸš€ Overview

This project visualizes **multi-head self-attention** in Transformer-based deep learning models (BERT).
By selecting a **token, layer, and attention head**, the user can observe how the model distributes attention across the input sentence.

ğŸ” Useful for:

* Deep Learning & NLP learning
* Transformer interpretability
* Research & education
* Viva / academic projects

---

## ğŸŒ Live Demo

ğŸ”— **App Link:** [https://multi-head-attention-output-viewer.streamlit.app/](https://multi-head-attention-output-viewer.streamlit.app/)
ğŸ”— **GitHub Repository:** [https://github.com/thanushshetty9353/DeepLearningAssignement](https://github.com/thanushshetty9353/DeepLearningAssignement)

---

## ğŸ“¸ Screenshots

### ğŸ  Home View

<img width="100%" alt="image" src="https://github.com/user-attachments/assets/8bac0a3f-45f5-403a-bc7b-eebe2b872bd9" />

### ğŸ”¥ Attention Heatmap

<img width="100%" alt="image" src="https://github.com/user-attachments/assets/be5e1159-6611-4a52-9f49-a4cb1974a536" />

### ğŸ” All Heads Comparison

<img width="1919" height="975" alt="image" src="https://github.com/user-attachments/assets/dc2d131b-9103-46fa-b070-c4803989dfdc" />

---

## âœ¨ Features

| Feature                             | Status |
| ----------------------------------- | ------ |
| Token-level attention visualization | âœ”      |
| Multi-head comparison               | âœ”      |
| Heatmap attention matrix            | âœ”      |
| Sentence-level visual highlighting  | âœ”      |
| Layer + Head selection              | âœ”      |
| Works for any sentence              | âœ”      |

---

## ğŸ› ï¸ Tech Stack

| Component     | Technology                       |
| ------------- | -------------------------------- |
| Language      | Python                           |
| Framework     | Streamlit                        |
| NLP Model     | BERT (Hugging Face Transformers) |
| Visualization | Plotly                           |
| Deployment    | Streamlit Cloud                  |

---

## ğŸ“‚ Project Structure

```
ğŸ“¦ Project
â”£ ğŸ“œ app.py
â”£ ğŸ“œ attention_utils.py
â”£ ğŸ“œ requirements.txt
â”£ ğŸ“‚ .streamlit
â”ƒ â”— ğŸ“œ config.toml
â”— ğŸ“œ README.md
```

---

## âš™ï¸ Installation & Running Locally

### ğŸ”¹ Clone the repository

```bash
git clone https://github.com/thanushshetty9353/DeepLearningAssignement.git
cd DeepLearningAssignement
```

### ğŸ”¹ Install dependencies

```bash
pip install -r requirements.txt
```

### ğŸ”¹ Run the application

```bash
streamlit run app.py
```

---

## ğŸ§  How it Works

1. Enter a sentence
2. Select **Layer**, **Head**, and **Token**
3. Visual output includes:

   * Sentence attention highlights
   * Full head heatmap
   * All-head comparison inside the layer
4. Understand how each attention head behaves differently

---

## ğŸ“š Concepts Covered

* Transformers
* Self-Attention
* Multi-Head Attention
* NLP Model Interpretability
* Deep Learning Visualization

---

## ğŸ‘¨â€ğŸ’» Author

**ğŸ’› Thanush Shetty**

ğŸ“ India

ğŸ”— GitHub: [https://github.com/thanushshetty9353](https://github.com/thanushshetty9353)

ğŸ”— LinkedIn: [https://www.linkedin.com/in/thanush-shetty-a49801298/](https://www.linkedin.com/in/thanush-shetty-a49801298/)

---

## â­ Contributions

Contributions, issues, and feature requests are welcome!
If you like this project, donâ€™t forget to **star â­ the repository**.

---

## ğŸ“„ License

This project is open-sourced under the **MIT License**.

---
